services:
  vllm:
    image: vllm/vllm-openai:v0.8.5.post1
    env_file:
      - .env
    entrypoint: ["/bin/sh", "-lc"]
    command:
      - |
        set -eu
        set -- --host 0.0.0.0 \
          --port 8000 \
          --model "${MODEL_NAME}" \
          --tensor-parallel-size "${GPU_COUNT}" \
          --gpu-memory-utilization "${GPU_MEMORY_UTILIZATION:-0.9}" \
          --max-model-len "${MAX_MODEL_LEN:-4096}"
        if [ -n "${VLLM_QUANTIZATION:-}" ]; then
          set -- "$@" --quantization "${VLLM_QUANTIZATION}"
        fi
        exec python3 -m vllm.entrypoints.openai.api_server "$@"
    ports:
      - "${VLLM_PORT:-8000}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 15s
      timeout: 10s
      retries: 40
      start_period: 180s

  translator:
    build:
      context: .
      dockerfile: Dockerfile.translator
    env_file:
      - .env
    environment:
      VLLM_BASE_URL: http://vllm:8000/v1
    depends_on:
      vllm:
        condition: service_healthy
    volumes:
      - ./:/app
    working_dir: /app
    command: >
      python run_translation_vllm.py
      --model ${MODEL_NAME}
      --parallel-requests ${PARALLEL_REQUESTS}
